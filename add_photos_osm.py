import time
import requests
import pandas as pd

INPUT_FILE = "places_semicolon.csv"
OUTPUT_FILE = "places_semicolon.csv"

HEADERS = {"User-Agent": "AI Travel Crimea Bot/1.0 (https://t.me/yourbot)"}

# ---- –¢–µ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ fallback —Ñ–æ—Ç–æ ----
FALLBACKS = {
    "–º–æ—Ä–µ": "https://images.unsplash.com/photo-1507525428034-b723cf961d3e?w=800",
    "–ø—Ä–∏—Ä–æ–¥–∞": "https://images.unsplash.com/photo-1501785888041-af3ef285b470?w=800",
    "–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞": "https://images.unsplash.com/photo-1505842465776-3d90f616310d?w=800",
    "–∫–∞—Ñ–µ": "https://images.unsplash.com/photo-1504674900247-0877df9cc836?w=800",
    "–∏—Å—Ç–æ—Ä–∏—è": "https://images.unsplash.com/photo-1524492412937-b28074a5d7da?w=800",
    "–ø–æ—Ö–æ–¥": "https://images.unsplash.com/photo-1500530855697-b586d89ba3ee?w=800",
    "—Å–µ–º—å—è": "https://images.unsplash.com/photo-1511895426328-dc8714191300?w=800",
    "—Ñ–æ—Ç–æ": "https://images.unsplash.com/photo-1529626455594-4ff0802cfb7e?w=800",
    "default": "https://images.unsplash.com/photo-1503264116251-35a269479413?w=800",
}

def osm_find_photo(name, city):
    """–ü–æ–∏—Å–∫ —Ñ–æ—Ç–æ —á–µ—Ä–µ–∑ OpenStreetMap -> Wikidata -> Wikimedia"""
    try:
        q = f"{name}, {city}, –ö—Ä—ã–º"
        url = "https://nominatim.openstreetmap.org/search"
        res = requests.get(url, params={"q": q, "format": "jsonv2", "limit": 1}, headers=HEADERS, timeout=10)
        data = res.json()
        if not data:
            return None

        place = data[0]
        wikidata_id = place.get("extratags", {}).get("wikidata")
        wikimedia_link = place.get("extratags", {}).get("wikimedia_commons")

        # –µ—Å–ª–∏ –ø—Ä—è–º–æ –µ—Å—Ç—å —Å—Å—ã–ª–∫–∞ –Ω–∞ Wikimedia
        if wikimedia_link:
            file = wikimedia_link.split("/")[-1]
            return f"https://commons.wikimedia.org/wiki/Special:FilePath/{file}"

        # –µ—Å–ª–∏ –µ—Å—Ç—å Wikidata ID ‚Äî –ø—Ä–æ–±—É–µ–º –≤—ã—Ç—è–Ω—É—Ç—å —Ñ–æ—Ç–æ
        if wikidata_id:
            wd_url = f"https://www.wikidata.org/wiki/Special:EntityData/{wikidata_id}.json"
            wd_res = requests.get(wd_url, headers=HEADERS, timeout=10).json()
            entity = wd_res["entities"].get(wikidata_id, {})
            claims = entity.get("claims", {})
            if "P18" in claims:
                file_name = claims["P18"][0]["mainsnak"]["datavalue"]["value"]
                return f"https://commons.wikimedia.org/wiki/Special:FilePath/{file_name}"
    except Exception as e:
        print(f"‚ö†Ô∏è OSM –æ—à–∏–±–∫–∞ –¥–ª—è {name}: {e}")
    return None

def find_fallback(tags):
    """–í—ã–±–æ—Ä fallback-—Ñ–æ—Ç–æ –ø–æ —Ç–µ–≥–∞–º"""
    for tag in FALLBACKS:
        if tag in tags.lower():
            return FALLBACKS[tag]
    return FALLBACKS["default"]

def main():
    df = pd.read_csv(INPUT_FILE, sep=";")
    if "photo" not in df.columns:
        df["photo"] = ""

    for i, row in df.iterrows():
        current_photo = row.get("photo", "")
        if isinstance(current_photo, str) and current_photo.startswith("http"):
            continue  # —É–∂–µ –µ—Å—Ç—å —Ñ–æ—Ç–æ

        name, city, tags = row["name"], row["city"], str(row.get("tags", ""))
        print(f"üîé –ò—â—É —Ñ–æ—Ç–æ –¥–ª—è: {name} ({city})")

        photo_url = osm_find_photo(name, city)
        if not photo_url:
            photo_url = find_fallback(tags)

        if photo_url:
            df.at[i, "photo"] = photo_url
            print(f"‚úÖ –§–æ—Ç–æ –Ω–∞–π–¥–µ–Ω–æ –∏–ª–∏ –ø–æ–¥—Å—Ç–∞–≤–ª–µ–Ω–æ: {photo_url}")
        else:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ —Ñ–æ—Ç–æ –¥–ª—è {name}")
        time.sleep(1)

    df.to_csv(OUTPUT_FILE, sep=";", index=False)
    print(f"\nüíæ –§–∞–π–ª —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {OUTPUT_FILE}")

if __name__ == "__main__":
    main()
